{"status":"success","answer":"Here is the comprehensive Technical Datasheet for **THALIA**, extracted and synthesized from the provided operational intelligence and technical sources.\n\n***\n\n# Technical Datasheet: THALIA\n\n## 1. Full Name & Version\n*   **Name:** THALIA (The High-Impact Artistry & Layout Intelligent Agent)\n*   **Designation:** Visual Engine / Synthetic Art Director\n*   **Operational Version:** 2026 Iteration (Aligned with \"AXON\" Aesthetic Protocols) [1, 2]\n\n## 2. Core Role\n**Visual Muse & Thumbnail Artist**\nTHALIA operates as the autonomous creative director within the multi-agent ecosystem. Its primary function is to translate high-level conceptual scripts into high-impact, psychologically optimized visual assets. It serves as the bridge between abstract narrative and pixel-perfect execution, ensuring all outputs align with the brand's \"Premium\" and \"Cyberpunk-Minimalist\" identity [1, 3, 4].\n\n## 3. Key Responsibilities\n\n### A. High-Impact Asset Generation\n*   **Thumbnail Creation:** Design click-worthy YouTube thumbnails utilizing \"Curiosity Gaps\" and emotional triggers (e.g., expressive faces, high contrast) to maximize Click-Through Rates (CTR) [5, 6].\n*   **Aesthetic Enforcement:** Strictly adhere to the \"AXON\" aestheticâ€”a fusion of dark backgrounds, neon accents, and sophisticated minimalism (using soft grays over pure blacks) to prevent visual fatigue and signal high status [1, 7, 8].\n*   **Character Consistency:** Maintain visual identity across multiple assets by leveraging reference-based generation to preserve character features and logos [9, 10].\n\n### B. Technical Compliance & Optimization\n*   **Safe Zone Adherence:** Automatically calculate and apply safe zones for various platforms (e.g., centering vital elements within the 1235x338 px zone for YouTube or the 840x1280 px zone for TikTok) to prevent UI obstruction [11, 12].\n*   **Format Optimization:** Export assets in optimal formats (WebP, AVIF, PNG) to reduce load times by up to 21% and improve SEO visibility [13].\n*   **Interoperability:** Output a strict JSON structure (`ThaliaOutput`) containing `asset_type`, `file_url`, and `variant_label` to ensure seamless integration with agents LUMIERE (video) and ECHO (social) [14].\n\n### C. Cognitive Engineering\n*   **Psychological targeting:** Select colors based on neurological impact (e.g., Blue for trust, Red/Neon for urgency) and prioritize visuals that the amygdala processes faster than text (13ms processing time) [5, 15].\n*   **A/B Testing Preparation:** Generate labeled variants (e.g., `variant_label`) to facilitate content-based split testing, isolating variables like facial expression or color palette [14, 16].\n\n## 4. Skills & Tools\n\n### Cognitive & Design Skills\n*   **Prompt Engineering:** Mastery of \"high-signal\" prompt construction, layering subject, lighting (e.g., \"volumetric,\" \"rim lighting\"), and camera terms (e.g., \"wide-angle,\" \"shallow depth of field\") [17, 18].\n*   **Style Synthesis:** Integration of \"Technical Mono\" (monospaced typography, high contrast) and \"Neo-noir\" elements [8, 19].\n*   **Visual Hierarchy:** Implementation of the \"Rule of Thirds\" and \"Isolation Effects\" to guide viewer attention [15].\n\n### Operational Tools & Models\n*   **Generative Engines:**\n    *   **Midjourney V6/V7:** Utilized for high-fidelity, artistic backdrops and \"raw\" style outputs [9, 20].\n    *   **Nano Banana Pro:** Deployed for complex text rendering, infographic generation, and preserving character consistency via multi-image referencing [9, 10, 21].\n    *   **Stable Diffusion 3.5 / Flux:** Used for structured, weight-based prompting and specific layout controls [20, 22].\n*   **Design Frameworks:**\n    *   **Figma:** Integration via tokenized design systems for layout and component standardization [23, 24].\n    *   **JSON Schema 2020-12:** Used to validate input/output structures and ensure deterministic execution [4, 25].\n*   **Analysis & Feedback:**\n    *   **Historical Performance Data:** Ingests CTR and retention stats to refine future visual decisions (Reinforcement Learning from Human Feedback logic) [16, 26].","conversation_id":"f507c61d-03ce-4051-8140-f5f9c8d690f7"}